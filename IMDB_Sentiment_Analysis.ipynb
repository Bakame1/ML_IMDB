{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b44ce39-46f0-446f-814c-c4e67251d04b",
   "metadata": {},
   "source": [
    "# Contextual Word Sentiment Classification\n",
    "\n",
    "This notebook implements a contextual word sentiment classification model using the IMDb dataset. \n",
    "The goal is to classify individual words as positive, negative, or neutral based on sentence-level sentiment labels, \n",
    "while incorporating the context of neighboring words.\n",
    "\n",
    "\n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99fb915-f1a3-4044-a2f5-b63b9e15ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c4b713-8e9f-4a23-9850-b8f5b8061015",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b3723c6-e6de-49da-8f1c-2889dea0240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation,Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten,GlobalMaxPooling1D,Conv1D, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab95678-c6fc-4ca2-a7fd-b1963472a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440abaf3-ce21-48a5-b413-47607fca3c3d",
   "metadata": {},
   "source": [
    "## TASK 1 : Load and preprocess the IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bf277-c6c9-4d17-a9d2-9f104f11cb0d",
   "metadata": {},
   "source": [
    "#### IMDb dataset can be loaded from torchtext or manually via pandas. In our case we decided to load it from a CSV file throught a pandas DataFrame.\n",
    "#### About the CSV file : LINK : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\r\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms\n",
    "#### Source : http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.bib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33287f5-d864-4b09-b0a0-fe077f9545a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################DISPLAY PART EXAMPLES ################################\n",
    "#Load data from a CSV file named \"IMDB Dataset.csv\" into a pandas DataFrame\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "#Display the first five rows of the DataFrame to get an initial look at the data\n",
    "df.head()\n",
    "\n",
    "#Display a concise summary of the DataFrame, including the data types of each column and the number of non-null entries\n",
    "df.info()\n",
    "\n",
    "#Total number of missing values per column\n",
    "df.isnull().sum()\n",
    "\n",
    "#Set the style of the plots to 'ggplot', which is a style that mimics the R ggplot2 package\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "#Use seaborn to create a count plot that shows the distribution of the 'sentiment' column in the DataFrame\n",
    "sns.countplot(data = df, x = df[\"sentiment\"])\n",
    "plt.show()\n",
    "\n",
    "# Display the number of reviews for each sentiment category using value_counts()\n",
    "df[\"sentiment\"].value_counts()\n",
    "\n",
    "# Access the third review text in the 'review' column (note: indexing starts at 0, so this is actually the fourth review)\n",
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f84693-ef16-4fcc-a3a9-fd90a5e15083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a sample review. It's a great movie, http://example.com, you should check it out!\n",
      "Normalized Text: this is a sample review it s a great movie you should check it out\n",
      "No Punctuation Text: this is a sample review it s a great movie you should check it out\n",
      "No Stopwords Text: sample review great movie check\n",
      "Stemmed Text: sampl review great movi check\n"
     ]
    }
   ],
   "source": [
    "#Ensure that the necessary NLTK resources are downloaded\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import string \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Using a set for faster lookup\n",
    "    words = text.split()\n",
    "    filtered_sentence = ' '.join([word for word in words if word not in stop_words])\n",
    "    return filtered_sentence\n",
    "\n",
    "# Function to normalize text by converting to lowercase, removing URLs, non-words, and extra spaces\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # r prefix for raw string to avoid escape issues\n",
    "    # Remove non-words and extra spaces\n",
    "    text = re.sub(r'\\W+', ' ', text)  # \\W matches any non-word character\n",
    "    text = re.sub(r'\\n', '', text)  # Remove newline characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text)  # Trim leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('', '', string.punctuation)  # Create a translation table for removing punctuation\n",
    "    return text.translate(table)\n",
    "\n",
    "# Function to perform stemming on text\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()  # Initialize the Porter Stemmer\n",
    "    words = text.split()\n",
    "    filtered_sentence = ' '.join([ps.stem(word) for word in words])\n",
    "    return filtered_sentence\n",
    "\n",
    "################# EXAMPLES ######################\n",
    "sample_text = \"This is a sample review. It's a great movie, http://example.com, you should check it out!\"\n",
    "print(\"Original Text:\", sample_text)\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalize_text(sample_text)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "# Remove punctuation\n",
    "no_punctuation_text = remove_punctuation(normalized_text)\n",
    "print(\"No Punctuation Text:\", no_punctuation_text)\n",
    "\n",
    "# Remove stopwords\n",
    "no_stopwords_text = remove_stopwords(no_punctuation_text)\n",
    "print(\"No Stopwords Text:\", no_stopwords_text)\n",
    "\n",
    "# Perform stemming\n",
    "stemmed_text = stemming(no_stopwords_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7612cbfc-672d-4ff5-9c4e-9ebebcc5d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = df[\"review\"]\n",
    "y = df['sentiment']\n",
    "\n",
    "one = OneHotEncoder()\n",
    "y = one.fit_transform(np.asarray(y).reshape(-1,1)).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52f7e1-bab5-48ff-933f-d6fb99d56bad",
   "metadata": {},
   "source": [
    "## TASK 2 : Implement tokenization and label propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f91ca3-dede-420a-b884-2d8372d764cd",
   "metadata": {},
   "source": [
    "#### Implement a function to calculate sentiment scores for each word based on sentence-level labels. \n",
    "#### The function should propagate labels to individual words and calculate a soft score for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84585960-7a22-434c-b1b5-89dcf3229af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Sentiment Scores:\n",
      "\n",
      "SCORE MEAN : \n",
      "Positive word  : 1\n",
      "Negative Word : 0 \n",
      "Cannot classify them(may delete them in the future)) : 0.5\n",
      "\n",
      "i: 0.5\n",
      "love: 1.0\n",
      "this: 0.5\n",
      "movie: 0.5\n",
      "hate: 0.0\n",
      "is: 0.5\n",
      "amazing: 1.0\n",
      "terrible: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example Sentences (X_train and X_test) and Labels (y_train)\n",
    "X_train = [\"I love this movie\", \"I hate this movie\", \"This movie is amazing\", \"This movie is terrible\"]\n",
    "y_train = [1, 0, 1, 0]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenize the texts\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Create a dictionary to store sentiment scores for each word\n",
    "word_sentiment_scores = {}\n",
    "\n",
    "# Assign sentiment scores to words based on sentence labels\n",
    "for sentence, label in zip(X_train, y_train):\n",
    "    # Tokenize the sentence\n",
    "    words = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    \n",
    "    # Calculate sentiment score for each word in the sentence\n",
    "    for word_index in words:\n",
    "        word = tokenizer.index_word.get(word_index)  # Get the word from the index\n",
    "        if word:\n",
    "            if word not in word_sentiment_scores:\n",
    "                word_sentiment_scores[word] = []\n",
    "            word_sentiment_scores[word].append(label)\n",
    "\n",
    "# Compute average sentiment score for each word\n",
    "for word in word_sentiment_scores:\n",
    "    sentiment_list = word_sentiment_scores[word]\n",
    "    # Calculate the soft score (average sentiment)\n",
    "    soft_score = np.mean(sentiment_list)\n",
    "    word_sentiment_scores[word] = soft_score\n",
    "\n",
    "# Display the sentiment scores for words\n",
    "print(\"Word Sentiment Scores:\\n\")\n",
    "print(\"SCORE MEAN : \\nPositive word  : 1\\nNegative Word : 0 \\nCannot classify them(may delete them in the future)) : 0.5\\n\")\n",
    "for word, score in word_sentiment_scores.items():\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "171aeb39-8b82-45b9-9ca0-fe952d4e66b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[342], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(word_sentiment_scores\u001b[38;5;241m.\u001b[39mitems[word])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(word_sentiment_scores.items[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e7800-9f2e-4e94-8e4b-e7cffa4bf55b",
   "metadata": {},
   "source": [
    "## Task 3: Prepare data for contextual learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c87931-a481-4c01-a606-5be9f1a8cc42",
   "metadata": {},
   "source": [
    "#### Implement a class to create a dataset with context windows. Each data point should include the word embedding for the target word, as well as an averaged embedding of the context words in a defined window size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eca06a79-1b47-4fde-97a7-eeeec9304293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded successfully!\n",
      "Target Word Embedding Shape: (100,)\n",
      "Context Word Embedding Shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the GloVe 6B 100D embeddings zip file\n",
    "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "# Send a request to get the zip file from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# If the request was successful, proceed\n",
    "if response.status_code == 200:\n",
    "    # Use io.BytesIO to handle the zip file in memory\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # Extract the specific file inside the zip\n",
    "        with zip_ref.open('glove.6B.100d.txt') as f:\n",
    "            # Load the embeddings into a dictionary\n",
    "            embedding_model = {}\n",
    "            for line in f:\n",
    "                # Decode each line\n",
    "                values = line.decode('utf-8').split()\n",
    "                word = values[0]\n",
    "                embedding_vector = np.array(values[1:], dtype='float32')\n",
    "                embedding_model[word] = embedding_vector\n",
    "\n",
    "            print(\"GloVe embeddings loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download the GloVe file. Status code: {response.status_code}\")\n",
    "\n",
    "# Example dataframe (this would be your dataset)\n",
    "data = {'text': [\"I love machine learning\", \"Natural language processing is great\", \"I enjoy data science\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Example word sentiment scores (can be based on your sentiment analysis)\n",
    "word_scores = {'love': 1, 'machine': 0, 'learning': 0, 'natural': 1, 'language': 1, 'processing': 0, 'great': 1, 'enjoy': 1, 'data': 1, 'science': 1}\n",
    "\n",
    "# Class definition for WordContextDataset\n",
    "class WordContextDataset:\n",
    "    def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
    "        self.df = df\n",
    "        self.word_scores = word_scores\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        words = text.split()\n",
    "        \n",
    "        # Iterate over words and create context windows\n",
    "        for i, target_word in enumerate(words):\n",
    "            # Check if the target word exists in the embedding model\n",
    "            if target_word in self.embedding_model:\n",
    "                target_embedding = self.embedding_model[target_word]\n",
    "                \n",
    "                # Find the context words within the window size\n",
    "                start = max(i - self.window_size, 0)\n",
    "                end = min(i + self.window_size + 1, len(words))\n",
    "                context_words = [words[j] for j in range(start, end) if j != i and words[j] in self.embedding_model]\n",
    "                \n",
    "                # Average the context word embeddings\n",
    "                context_embedding = np.mean([self.embedding_model[word] for word in context_words], axis=0) if context_words else np.zeros_like(target_embedding)\n",
    "                \n",
    "                return target_embedding, context_embedding\n",
    "\n",
    "# Create the dataset\n",
    "window_size = 2\n",
    "dataset = WordContextDataset(df, word_scores, embedding_model, window_size)\n",
    "\n",
    "# Example of accessing a data point\n",
    "target_embedding, context_embedding = dataset[0]\n",
    "print(\"Target Word Embedding Shape:\", target_embedding.shape)  # Should print (100,)\n",
    "print(\"Context Word Embedding Shape:\", context_embedding.shape)  # Should print (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "486ca1c5-cca1-4b43-b162-3cc4ccd002fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.WordContextDataset object at 0x0000020CBC3AF0E0>\n"
     ]
    }
   ],
   "source": [
    "len(dataset)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46030eb-634d-4e40-a583-0aa0c0d7b59d",
   "metadata": {},
   "source": [
    "### Task 4: Define and train the model\n",
    "#### Define a neural network for sentiment classification using PyTorch. The network should take an input vector of concatenated word and context embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a528e192-3903-4a87-bfbc-9e38dae93660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Define and train the model\n",
    "# Define a neural network for sentiment classification using PyTorch.\n",
    "# The network should take an input vector of concatenated word and context embeddings.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 100)  # First fully connected layer\n",
    "        self.relu = nn.ReLU()  # ReLU activation\n",
    "        self.fc2 = nn.Linear(100, 100)  # Output layer (binary sentiment classification)\n",
    "        self.sigmoid = nn.Sigmoid()  # Sigmoid for binary classification\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class WordContextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
    "        self.df = df\n",
    "        self.word_scores = word_scores\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "   \n",
    "def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx]['text']\n",
    "    words = text.split()\n",
    "\n",
    "    target_embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for i, target_word in enumerate(words):\n",
    "        if target_word in self.embedding_model:\n",
    "            target_embedding = self.embedding_model[target_word]  # Shape: (100,)\n",
    "            context_embedding = np.zeros_like(target_embedding)  # Shape: (100,)\n",
    "\n",
    "            context_words = [words[j] for j in range(max(0, i - self.window_size), min(len(words), i + self.window_size + 1)) if j != i and words[j] in self.embedding_model]\n",
    "            if context_words:\n",
    "                context_embeddings = np.array([self.embedding_model[word] for word in context_words])  # Shape: (num_context_words, 100)\n",
    "                context_embedding = np.mean(context_embeddings, axis=0)  # Shape: (100,)\n",
    "\n",
    "            label = self.word_scores.get(target_word, 0)  # Default 0 if not in word_scores\n",
    "\n",
    "            embedding_vector = np.concatenate([target_embedding, context_embedding])  # Shape: (200,)\n",
    "            target_embeddings.append(embedding_vector)\n",
    "            labels.append(label)\n",
    "\n",
    "    if not target_embeddings:\n",
    "        return torch.zeros((1, 200)), torch.zeros((1, 1))  # Default for empty\n",
    "\n",
    "    target_embeddings = torch.tensor(np.array(target_embeddings), dtype=torch.float32)\n",
    "    labels = torch.tensor(np.array(labels), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    return target_embeddings, labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "323527c1-b9ba-4560-9762-473a11efe03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [2/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [3/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [4/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [5/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [6/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [7/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [8/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [9/10], Loss: 0.4773, Accuracy: 0.00%\n",
      "Epoch [10/10], Loss: 0.4773, Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "input_dim = 200  # This should be the sum of the dimensions of your word and context embeddings\n",
    "\n",
    "model = SentimentClassifier(input_dim)\n",
    "\n",
    "# Create a DataLoader instance for your dataset\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for target_embeddings, labels in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(torch.unsqueeze(torch.cat([target_embeddings[0],torch.tensor(context_embedding)]),dim=0))\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        predicted = (outputs >= 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    if total == 0:\n",
    "        print(\"No valid samples processed in epoch. Check data filtering and embedding dimensions.\")\n",
    "    else:\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "7c9cc0e7-94e9-4c28-b193-6137d996c803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.5975e-01,  5.5833e-01,  5.7986e-01, -2.1361e-01,  1.3084e-01,\n",
       "          9.4385e-01, -4.2817e-01, -3.7420e-01, -9.4499e-02, -4.3344e-01,\n",
       "         -2.0937e-01,  3.4702e-01,  8.2516e-02,  7.9735e-01,  1.6606e-01,\n",
       "         -2.6878e-01,  5.8830e-01,  6.7397e-01, -4.9965e-01,  1.4764e+00,\n",
       "          5.5261e-01,  2.5295e-02, -1.6068e-01, -1.3878e-01,  4.8686e-01,\n",
       "          1.1420e+00,  5.6195e-02, -7.3306e-01,  8.6932e-01, -3.5892e-01,\n",
       "         -5.1877e-01,  9.0402e-01,  4.9249e-01, -1.4915e-01,  4.8493e-02,\n",
       "          2.6096e-01,  1.1352e-01,  4.1275e-01,  5.3803e-01, -4.4950e-01,\n",
       "          8.5733e-02,  9.1184e-02,  5.0177e-03, -3.4645e-01, -1.1058e-01,\n",
       "         -2.2235e-01, -6.5290e-01, -5.1838e-02,  5.3791e-01, -8.1040e-01,\n",
       "         -1.8253e-01,  2.4194e-01,  5.4855e-01,  8.7731e-01,  2.2165e-01,\n",
       "         -2.7124e+00,  4.9405e-01,  4.4703e-01,  5.5882e-01,  2.6076e-01,\n",
       "          2.3760e-01,  1.0668e+00, -5.6971e-01, -6.4960e-01,  3.3511e-01,\n",
       "          3.4609e-01,  1.1033e+00,  8.5261e-02,  2.4847e-02, -4.5453e-01,\n",
       "          7.7012e-02,  2.1321e-01,  1.0444e-01,  6.7157e-02, -3.4261e-01,\n",
       "          8.5534e-01,  1.3361e-01, -4.3296e-01, -5.6726e-01, -2.1348e-01,\n",
       "         -3.3277e-01,  3.4351e-01,  3.2164e-01,  4.4527e-01, -1.3208e+00,\n",
       "         -1.3270e-01, -7.0820e-01, -4.8472e-01, -6.9396e-01, -2.6080e-01,\n",
       "         -4.7099e-01, -5.7492e-02,  9.3587e-02,  4.0006e-01, -4.3419e-01,\n",
       "         -2.7364e-01, -7.7017e-01, -8.4028e-01, -1.5620e-03,  6.2223e-01,\n",
       "         -2.7650e-03,  5.9649e-01, -3.3096e-01,  2.8456e-01, -1.2272e-01,\n",
       "          2.7970e-01,  3.5571e-01,  1.0608e-01,  2.9469e-01,  3.9728e-01,\n",
       "          1.5981e-01, -4.8243e-01,  3.5343e-02, -1.1424e-01,  2.5616e-01,\n",
       "         -6.0072e-02,  2.5134e-01,  2.7191e-01, -4.2963e-01,  2.5399e-01,\n",
       "         -4.1906e-01, -1.3424e-01,  1.7332e-01, -3.7442e-01, -1.0836e-01,\n",
       "          2.9090e-02, -1.6663e-01, -3.6320e-02, -2.2079e-01,  3.3805e-01,\n",
       "         -2.6669e-01,  8.6973e-01, -3.1351e-01, -2.4218e-01,  3.2380e-01,\n",
       "         -6.5730e-01, -2.7280e-01, -1.0429e-01,  6.1228e-01, -5.1671e-01,\n",
       "         -1.7637e-01, -4.2350e-02, -3.4505e-01, -1.6206e-01, -4.6755e-01,\n",
       "         -1.1611e-01, -4.5445e-02, -7.6835e-02,  4.2538e-02, -5.2233e-01,\n",
       "         -1.5221e-01,  5.9865e-02,  1.9416e-01,  9.4172e-01,  5.8777e-01,\n",
       "         -1.6865e+00,  2.3220e-01,  1.9922e-01,  1.2753e+00,  4.2872e-01,\n",
       "          4.4250e-01,  5.1717e-01, -1.7648e-01,  3.5891e-01, -1.2504e-01,\n",
       "          3.4848e-01,  1.9511e-01, -3.0681e-01,  1.7282e-01,  3.6222e-01,\n",
       "         -3.6937e-01,  9.3235e-02,  3.3196e-01, -2.6541e-01,  7.7930e-02,\n",
       "          4.1125e-01,  2.1115e-02,  7.8481e-03, -4.0120e-01, -7.5130e-02,\n",
       "          4.7400e-01,  1.8418e-01, -5.9826e-01,  6.3605e-03, -1.6152e+00,\n",
       "          6.0702e-01,  3.7050e-01, -7.3632e-01, -1.3930e-01, -1.8246e-01,\n",
       "          4.9770e-02,  2.1116e-01,  3.9715e-01,  7.0364e-01,  5.8695e-02,\n",
       "         -4.6452e-01, -1.5071e-01, -6.8875e-01,  1.0976e+00,  2.7253e-01]])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unsqueeze(torch.cat([target_embeddings[0],torch.tensor(context_embedding)]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ab4fb592-b761-4ba7-bc80-b3a891e3abd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3122,  0.5653,  0.4127, -0.1864,  0.3486, -0.4635, -0.1859,  0.5984,\n",
       "          0.2524,  0.1091, -0.1508, -0.2443,  0.1986,  0.0171,  0.1031, -0.0911,\n",
       "          0.5399,  0.0620,  0.4683,  0.2777, -0.3438,  0.1653,  0.3546,  0.0162,\n",
       "         -0.0231,  0.1783,  0.2668, -0.0809, -0.4016,  0.1789, -0.6087,  0.6251,\n",
       "         -0.0326, -0.1080,  0.3249, -0.0768,  0.0630,  0.0367,  0.0676, -0.8441,\n",
       "         -0.1399, -0.6493,  0.1035, -0.2465,  0.0601,  0.0466,  0.3758, -0.3718,\n",
       "         -0.0754, -0.3570,  0.2217, -0.4927,  0.1683,  1.1943, -0.4533, -1.5591,\n",
       "         -0.4643, -0.4954,  1.8850,  0.2510,  0.1794,  0.2626,  0.2081, -0.2362,\n",
       "          0.7154,  0.0278,  0.5407, -0.1194,  0.4990, -0.0771, -0.3130,  0.0344,\n",
       "          0.1003,  0.0170,  0.6875, -0.0035, -0.0857,  0.1669, -0.8301,  0.3244,\n",
       "          0.4908,  0.1497, -1.0322,  0.2345, -1.5461,  0.3266,  0.6923, -0.6434,\n",
       "          0.2235, -0.4955, -0.2107, -0.0322, -0.2773,  0.1429,  0.2485,  0.0200,\n",
       "          0.1219, -0.9912,  1.0791,  0.1974]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "f0d526b1-cc15-4254-bf7b-60c1b8397e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0.,\n",
       "         1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
       "         1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
       "         1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "         0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
       "         0., 0., 0., 1., 0., 1., 1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "030e5088-a46a-4eee-beaf-216f42eb3c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCELoss()"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c4347aca-620c-43dc-b717-10ca02de2fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100])"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1395bbb6-e470-4503-8692-a41964fb492d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beed1f0-9b85-4121-801f-b8217996f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate the model\n",
    "# Evaluate the trained model on a validation set.\n",
    "# Use metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df935c85-b28c-4b88-9d14-9104095272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to evaluate the model:\n",
    "# with torch.no_grad():\n",
    "#     # Predict on validation data and calculate metrics\n",
    "#     pass\n",
    "\n",
    "# Optional: Experiment with hyperparameters or model architecture to improve performance.\n",
    "# Examples: Try different window sizes, embedding dimensions, or additional layers in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1150845-aa73-46b8-a0e1-b6bc8bb4a164",
   "metadata": {},
   "source": [
    "#### Difficulties Encountered\n",
    "- One of the main difficulties was the technical issue with TorchText and Python version compatibility. This required a shift in strategy to use Kaggle and Pandas for data loading, which was not the initial plan.\n",
    "- Problem of datas that had different shape\n",
    "- Problem related to keras : in the function to fit the model, we are using sequential model and it says sequential has no outputs yet. Therefore, we tried to look for the issue on stackoverflow and github but we did not get response so we continued to debug it.\n",
    "\n",
    "#### Steps taken to alleviate difficulties\n",
    "To address the TorchText and Python version compatibility issue, we opted to use Kaggle to directly download the dataset and then import it into Pandas for preprocessing. This change in strategy allowed us to bypass the initial dependency from TorchText. For the challenge of data with varying shapes, I implemented data preprocessing techniques such as padding and truncation to normalize the input data, ensuring that all sequences were of equal length and suitable for model training. The website stack helped also while we were facing errors.\n",
    "\n",
    "#### General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "For this task, we built a model that could classify the sentiment of individual words based on the sentiment of the sentences they appear in. To do so, we first prepared the data by tokenizing the text into words and sentences, and then padded the sentences to a uniform length to standardize the input for the model. Then, we developed a neural network model designed to process sequences of words and learn the contextual relationships that influence word sentiment. The model was trained on the IMDb dataset, using the sentence-level sentiment labels to teach it to classify words as positive, negative, or neutral. After training, the model was evaluated to assess its ability to accurately classify word sentiments in new sentences.\n",
    "\n",
    "#### Potential limitations of our approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- **Data Bias**: The IMDb dataset may contain biases that could affect the model's performance on different types of data or under slightly different conditions.\n",
    "- **Context Length**: The model might struggle with longer contexts or sentences where the relationship between words and sentiment is more complex.\n",
    "- **Vocabulary Size**: Limitations in the model's vocabulary size could lead to misclassifications of words that are not well-represented in the training data.\n",
    "\n",
    "#### Extensions and Future Work\n",
    "An interesting extension of this work could involve exploring different neural network architectures, such as transformers, which are known for their ability to handle long-range dependencies in text. Additionally, incorporating multimodal data, such as movie scenes or audio, could provide richer context for sentiment analysis. This approach could also be extended to detect hate speech or discriminatory comments on social media platforms more effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b95bdc-1e35-4401-8887-f75562938879",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
