{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b44ce39-46f0-446f-814c-c4e67251d04b",
   "metadata": {},
   "source": [
    "# Contextual Word Sentiment Classification\n",
    "\n",
    "This notebook implements a contextual word sentiment classification model using the IMDb dataset. \n",
    "The goal is to classify individual words as positive, negative, or neutral based on sentence-level sentiment labels, \n",
    "while incorporating the context of neighboring words.\n",
    "\n",
    "\n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99fb915-f1a3-4044-a2f5-b63b9e15ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4b713-8e9f-4a23-9850-b8f5b8061015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3723c6-e6de-49da-8f1c-2889dea0240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation,Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Flatten,GlobalMaxPooling1D,Conv1D, Embedding, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ab95678-c6fc-4ca2-a7fd-b1963472a75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440abaf3-ce21-48a5-b413-47607fca3c3d",
   "metadata": {},
   "source": [
    "## TASK 1 : Load and preprocess the IMDb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4bf277-c6c9-4d17-a9d2-9f104f11cb0d",
   "metadata": {},
   "source": [
    "#### IMDb dataset can be loaded from torchtext or manually via pandas. In our case we decided to load it from a CSV file throught a pandas DataFrame.\n",
    "#### About the CSV file : LINK : https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?resource=download\n",
    "IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\r\n",
    "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training and 25,000 for testing. So, predict the number of positive and negative reviews using either classification or deep learning algorithms\n",
    "#### Source : http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.bib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33287f5-d864-4b09-b0a0-fe077f9545a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I\\'d laughed at one of Woody\\'s comedies in years (dare I say a decade?). While I\\'ve never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###################DISPLAY PART EXAMPLES ################################\n",
    "#Load data from a CSV file named \"IMDB Dataset.csv\" into a pandas DataFrame\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "#Display the first five rows of the DataFrame to get an initial look at the data\n",
    "df.head()\n",
    "\n",
    "#Display a concise summary of the DataFrame, including the data types of each column and the number of non-null entries\n",
    "df.info()\n",
    "\n",
    "#Total number of missing values per column\n",
    "df.isnull().sum()\n",
    "\n",
    "#Set the style of the plots to 'ggplot', which is a style that mimics the R ggplot2 package\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "#Use seaborn to create a count plot that shows the distribution of the 'sentiment' column in the DataFrame\n",
    "sns.countplot(data = df, x = df[\"sentiment\"])\n",
    "plt.show()\n",
    "\n",
    "# Display the number of reviews for each sentiment category using value_counts()\n",
    "df[\"sentiment\"].value_counts()\n",
    "\n",
    "# Access the third review text in the 'review' column (note: indexing starts at 0, so this is actually the fourth review)\n",
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67f84693-ef16-4fcc-a3a9-fd90a5e15083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: This is a sample review. It's a great movie, http://example.com, you should check it out!\n",
      "Normalized Text: this is a sample review it s a great movie you should check it out\n",
      "No Punctuation Text: this is a sample review it s a great movie you should check it out\n",
      "No Stopwords Text: sample review great movie check\n",
      "Stemmed Text: sampl review great movi check\n"
     ]
    }
   ],
   "source": [
    "#Ensure that the necessary NLTK resources are downloaded\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "import string \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Function to remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))  # Using a set for faster lookup\n",
    "    words = text.split()\n",
    "    filtered_sentence = ' '.join([word for word in words if word not in stop_words])\n",
    "    return filtered_sentence\n",
    "\n",
    "# Function to normalize text by converting to lowercase, removing URLs, non-words, and extra spaces\n",
    "def normalize_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # r prefix for raw string to avoid escape issues\n",
    "    # Remove non-words and extra spaces\n",
    "    text = re.sub(r'\\W+', ' ', text)  # \\W matches any non-word character\n",
    "    text = re.sub(r'\\n', '', text)  # Remove newline characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r'^\\s+|\\s+$', '', text)  # Trim leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    table = str.maketrans('', '', string.punctuation)  # Create a translation table for removing punctuation\n",
    "    return text.translate(table)\n",
    "\n",
    "# Function to perform stemming on text\n",
    "def stemming(text):\n",
    "    ps = PorterStemmer()  # Initialize the Porter Stemmer\n",
    "    words = text.split()\n",
    "    filtered_sentence = ' '.join([ps.stem(word) for word in words])\n",
    "    return filtered_sentence\n",
    "\n",
    "################# EXAMPLES ######################\n",
    "sample_text = \"This is a sample review. It's a great movie, http://example.com, you should check it out!\"\n",
    "print(\"Original Text:\", sample_text)\n",
    "\n",
    "# Normalize the text\n",
    "normalized_text = normalize_text(sample_text)\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "\n",
    "# Remove punctuation\n",
    "no_punctuation_text = remove_punctuation(normalized_text)\n",
    "print(\"No Punctuation Text:\", no_punctuation_text)\n",
    "\n",
    "# Remove stopwords\n",
    "no_stopwords_text = remove_stopwords(no_punctuation_text)\n",
    "print(\"No Stopwords Text:\", no_stopwords_text)\n",
    "\n",
    "# Perform stemming\n",
    "stemmed_text = stemming(no_stopwords_text)\n",
    "print(\"Stemmed Text:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7612cbfc-672d-4ff5-9c4e-9ebebcc5d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "X = df[\"review\"]\n",
    "y = df['sentiment']\n",
    "\n",
    "one = OneHotEncoder()\n",
    "y = one.fit_transform(np.asarray(y).reshape(-1,1)).toarray()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e52f7e1-bab5-48ff-933f-d6fb99d56bad",
   "metadata": {},
   "source": [
    "## TASK 2 : Implement tokenization and label propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f91ca3-dede-420a-b884-2d8372d764cd",
   "metadata": {},
   "source": [
    "#### Implement a function to calculate sentiment scores for each word based on sentence-level labels. \n",
    "#### The function should propagate labels to individual words and calculate a soft score for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84585960-7a22-434c-b1b5-89dcf3229af3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Sentiment Scores:\n",
      "\n",
      "SCORE MEAN : \n",
      "Positive word  : 1\n",
      "Negative Word : 0 \n",
      "Cannot classify them(may delete them in the future)) : 0.5\n",
      "\n",
      "i: 0.5\n",
      "love: 1.0\n",
      "this: 0.5\n",
      "movie: 0.5\n",
      "hate: 0.0\n",
      "is: 0.5\n",
      "amazing: 1.0\n",
      "terrible: 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Example Sentences (X_train and X_test) and Labels (y_train)\n",
    "X_train = [\"I love this movie\", \"I hate this movie\", \"This movie is amazing\", \"This movie is terrible\"]\n",
    "y_train = [1, 0, 1, 0]  # 1 for positive sentiment, 0 for negative sentiment\n",
    "\n",
    "vocab_size = 10000\n",
    "max_length = 50\n",
    "oov_tok = '<OOV>'\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenize the texts\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Create a dictionary to store sentiment scores for each word\n",
    "word_sentiment_scores = {}\n",
    "\n",
    "# Assign sentiment scores to words based on sentence labels\n",
    "for sentence, label in zip(X_train, y_train):\n",
    "    # Tokenize the sentence\n",
    "    words = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    \n",
    "    # Calculate sentiment score for each word in the sentence\n",
    "    for word_index in words:\n",
    "        word = tokenizer.index_word.get(word_index)  # Get the word from the index\n",
    "        if word:\n",
    "            if word not in word_sentiment_scores:\n",
    "                word_sentiment_scores[word] = []\n",
    "            word_sentiment_scores[word].append(label)\n",
    "\n",
    "# Compute average sentiment score for each word\n",
    "for word in word_sentiment_scores:\n",
    "    sentiment_list = word_sentiment_scores[word]\n",
    "    # Calculate the soft score (average sentiment)\n",
    "    soft_score = np.mean(sentiment_list)\n",
    "    word_sentiment_scores[word] = soft_score\n",
    "\n",
    "# Display the sentiment scores for words\n",
    "print(\"Word Sentiment Scores:\\n\")\n",
    "print(\"SCORE MEAN : \\nPositive word  : 1\\nNegative Word : 0 \\nCannot classify them(may delete them in the future)) : 0.5\\n\")\n",
    "for word, score in word_sentiment_scores.items():\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5e7800-9f2e-4e94-8e4b-e7cffa4bf55b",
   "metadata": {},
   "source": [
    "## Task 3: Prepare data for contextual learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c87931-a481-4c01-a606-5be9f1a8cc42",
   "metadata": {},
   "source": [
    "#### Implement a class to create a dataset with context windows. Each data point should include the word embedding for the target word, as well as an averaged embedding of the context words in a defined window size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eca06a79-1b47-4fde-97a7-eeeec9304293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded successfully!\n",
      "Target Word Embedding Shape: (100,)\n",
      "Context Word Embedding Shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the GloVe 6B 100D embeddings zip file\n",
    "url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "# Send a request to get the zip file from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# If the request was successful, proceed\n",
    "if response.status_code == 200:\n",
    "    # Use io.BytesIO to handle the zip file in memory\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        # Extract the specific file inside the zip\n",
    "        with zip_ref.open('glove.6B.100d.txt') as f:\n",
    "            # Load the embeddings into a dictionary\n",
    "            embedding_model = {}\n",
    "            for line in f:\n",
    "                # Decode each line\n",
    "                values = line.decode('utf-8').split()\n",
    "                word = values[0]\n",
    "                embedding_vector = np.array(values[1:], dtype='float32')\n",
    "                embedding_model[word] = embedding_vector\n",
    "\n",
    "            print(\"GloVe embeddings loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Failed to download the GloVe file. Status code: {response.status_code}\")\n",
    "\n",
    "# Example dataframe (this would be your dataset)\n",
    "data = {'text': [\"I love machine learning\", \"Natural language processing is great\", \"I enjoy data science\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Example word sentiment scores (can be based on your sentiment analysis)\n",
    "word_scores = {'love': 1, 'machine': 0, 'learning': 0, 'natural': 1, 'language': 1, 'processing': 0, 'great': 1, 'enjoy': 1, 'data': 1, 'science': 1}\n",
    "\n",
    "# Class definition for WordContextDataset\n",
    "class WordContextDataset:\n",
    "    def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
    "        self.df = df\n",
    "        self.word_scores = word_scores\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['text']\n",
    "        words = text.split()\n",
    "        \n",
    "        # Iterate over words and create context windows\n",
    "        for i, target_word in enumerate(words):\n",
    "            # Check if the target word exists in the embedding model\n",
    "            if target_word in self.embedding_model:\n",
    "                target_embedding = self.embedding_model[target_word]\n",
    "                \n",
    "                # Find the context words within the window size\n",
    "                start = max(i - self.window_size, 0)\n",
    "                end = min(i + self.window_size + 1, len(words))\n",
    "                context_words = [words[j] for j in range(start, end) if j != i and words[j] in self.embedding_model]\n",
    "                \n",
    "                # Average the context word embeddings\n",
    "                context_embedding = np.mean([self.embedding_model[word] for word in context_words], axis=0) if context_words else np.zeros_like(target_embedding)\n",
    "                \n",
    "                return target_embedding, context_embedding\n",
    "\n",
    "# Create the dataset\n",
    "window_size = 2\n",
    "dataset = WordContextDataset(df, word_scores, embedding_model, window_size)\n",
    "\n",
    "# Example of accessing a data point\n",
    "target_embedding, context_embedding = dataset[0]\n",
    "print(\"Target Word Embedding Shape:\", target_embedding.shape)  # Should print (100,)\n",
    "print(\"Context Word Embedding Shape:\", context_embedding.shape)  # Should print (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46030eb-634d-4e40-a583-0aa0c0d7b59d",
   "metadata": {},
   "source": [
    "### Task 4: Define and train the model\n",
    "#### Define a neural network for sentiment classification using PyTorch. The network should take an input vector of concatenated word and context embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eaf77713-74e7-40c7-b50d-5f42dd28466b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `validation_split` is only supported for tensors or NumPy arrays.Found incompatible type in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_train)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Now y_train_encoded is numeric labels, which can be used for training\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_encoded, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_slicing.py:472\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m    470\u001b[0m unsplitable \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m can_slice_array(t)]\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsplitable:\n\u001b[1;32m--> 472\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    473\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `validation_split` is only supported \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor tensors or NumPy arrays.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    475\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound incompatible type in the input: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munsplitable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[1;31mValueError\u001b[0m: Argument `validation_split` is only supported for tensors or NumPy arrays.Found incompatible type in the input: [<class 'str'>, <class 'str'>, <class 'str'>, <class 'str'>]"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 64, input_length=max_length))\n",
    "    model.add(LSTM(64, return_sequences=True))\n",
    "    model.add(Dropout(0.2)) # Add dropout regularization\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2)) # Add dropout regularization\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2)) # Add dropout regularization\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3) # Apply early stop\n",
    "    return model, early_stopping\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming y_train are string category labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Now y_train_encoded is numeric labels, which can be used for training\n",
    "history = model.fit(X_train, y_train_encoded, epochs=15, batch_size=batch_size, validation_split=0.1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "29d625da-c09a-443b-8c2c-795e01d750d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# class SentimentClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SentimentClassifier, self).__init__()\n",
    "#         # Your code here\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Your code here\n",
    "#         pass\n",
    "\n",
    "# Implement a training loop to train the model on the dataset created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beed1f0-9b85-4121-801f-b8217996f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate the model\n",
    "# Evaluate the trained model on a validation set.\n",
    "# Use metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df935c85-b28c-4b88-9d14-9104095272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to evaluate the model:\n",
    "# with torch.no_grad():\n",
    "#     # Predict on validation data and calculate metrics\n",
    "#     pass\n",
    "\n",
    "# Optional: Experiment with hyperparameters or model architecture to improve performance.\n",
    "# Examples: Try different window sizes, embedding dimensions, or additional layers in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
