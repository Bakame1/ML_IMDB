{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b44ce39-46f0-446f-814c-c4e67251d04b",
   "metadata": {},
   "source": [
    "# Contextual Word Sentiment Classification\n",
    "\n",
    "This notebook implements a contextual word sentiment classification model using the IMDb dataset. \n",
    "The goal is to classify individual words as positive, negative, or neutral based on sentence-level sentiment labels, \n",
    "while incorporating the context of neighboring words.\n",
    "\n",
    "\n",
    "**Important**: At the end you should write a report of adequate size, which will probably mean at least half a page. In the report you should describe how you approached the task. You should describe:\n",
    "- Encountered difficulties (due to the method, e.g. \"not enough training samples to converge\", not technical like \"I could not install a package over pip\")\n",
    "- Steps taken to alleviate difficulties\n",
    "- General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "- Potential limitations of your approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- If you have an idea how this could be extended in an interesting way, describe it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f99fb915-f1a3-4044-a2f5-b63b9e15ab72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/waelbenslima/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')  # For tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c77722-376a-495c-a570-cc64e327bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "#TASK 1 TEST FINAL DONE DONT TOUCH \n",
    "from keras.datasets import imdb\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c7aff9a-5a61-4ab4-9920-c0981c05b130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d92905-2fc7-445c-bbe5-35392d00248e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INDEX_FROM = 3\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {key:(value+INDEX_FROM) for key,value in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0    # the padding token\n",
    "word_index[\"<START>\"] = 1  # the starting token\n",
    "word_index[\"<UNK>\"] = 2    # the unknown token\n",
    "reverse_word_index = {value:key for key, value in word_index.items()}\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "decode_review(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8132a0-314d-4c8a-83cd-58fb9a00443a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 train sequences\n",
      "5000 val sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000 \n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words= vocab_size)\n",
    "\n",
    "X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_val), 'val sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612cbfc-672d-4ff5-9c4e-9ebebcc5d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tip. You can get the dataset from torchtext but the package is old and needs pytorch version 2.2 to work\n",
    "## If you want to use it choose your versions like this: \n",
    "## !pip install -U torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu121 torchtext\n",
    "# from torchtext.datasets import IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da4c285-3046-4bb1-9ae7-f3e77829f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK 2 & Maybe 3 ? TEST FINAL DONT TOUCH \n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "maximum_sequence_length = 500 # maximum length of all review sequences\n",
    "\n",
    "X_train = pad_sequences(X_train, value= word_index[\"<PAD>\"], padding= 'post', maxlen= maximum_sequence_length)\n",
    "X_val = pad_sequences(X_val, value= word_index[\"<PAD>\"], padding= 'post', maxlen= maximum_sequence_length)\n",
    "X_test = pad_sequences(X_test, value= word_index[\"<PAD>\"], padding= 'post', maxlen= maximum_sequence_length)\n",
    "\n",
    "print('X_train shape:', X_train.shape) # (n_samples, n_timesteps)\n",
    "print('X_val shape:', X_val.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb740ac2-3225-4d8f-8143-dd8f2f83d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1bedc-2f32-4b0c-920d-300c599dd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "\n",
    "# Define embedding dimension and other constants\n",
    "embedding_dim = 16\n",
    "vocab_size = 10000  # Example value\n",
    "maximum_sequence_length = 100  # Example value\n",
    "\n",
    "# Define dummy data for testing\n",
    "X_train = np.random.randint(1, vocab_size, size=(1000, maximum_sequence_length))\n",
    "y_train = np.random.randint(0, 2, size=(1000,))\n",
    "X_val = np.random.randint(1, vocab_size, size=(200, maximum_sequence_length))\n",
    "y_val = np.random.randint(0, 2, size=(200,))\n",
    "\n",
    "# Define the model-building function\n",
    "def create_model(filters=64, kernel_size=3, strides=1, units=256, \n",
    "                 optimizer='adam', rate=0.25, kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(vocab_size, embedding_dim))\n",
    "    # Convolutional Layer(s)\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                     padding='same', activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # Dense layer(s)\n",
    "    model.add(Dense(units=units, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(rate))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Ensure binary classification output\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "model = KerasClassifier(model=create_model, verbose=1)\n",
    "\n",
    "\n",
    "# Define hyperparameter grid with 'model__' prefix\n",
    "param_grid = {\n",
    "    'model__filters': [64, 128],\n",
    "    'model__kernel_size': [3, 5],\n",
    "    'model__strides': [1],\n",
    "    'model__units': [128, 256],\n",
    "    'model__optimizer': ['adam', 'rmsprop'],\n",
    "    'model__rate': [0.25, 0.5],\n",
    "    'model__kernel_initializer': ['glorot_uniform', 'he_normal'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Exhaustive Grid Search\n",
    "grid = ParameterGrid(param_grid)\n",
    "param_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Set model parameters\n",
    "    model.set_params(**params)\n",
    "    \n",
    "    # Early stopping\n",
    "    earlystopper = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1)\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopper]\n",
    "    )\n",
    "    \n",
    "    # Store the validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    param_scores.append(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "# Select the best parameters\n",
    "best_index = np.argmax(param_scores)\n",
    "best_params = list(grid)[best_index]\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {param_scores[best_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84585960-7a22-434c-b1b5-89dcf3229af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Implement tokenization and label propagation\n",
    "# Implement a function to calculate sentiment scores for each word based on sentence-level labels.\n",
    "# The function should propagate labels to individual words and calculate a soft score for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca06a79-1b47-4fde-97a7-eeeec9304293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: You can use word_tokenize for tokenization\n",
    "# Hint: You can use a dictionary to store counts of positive and negative labels for each word.\n",
    "\n",
    "# Task 3: Prepare data for contextual learning\n",
    "# Implement a class to create a dataset with context windows. \n",
    "# Each data point should include the word embedding for the target word, \n",
    "# as well as an averaged embedding of the context words in a defined window size.\n",
    "\n",
    "# Use a pre-trained embedding model like GloVe. Download the embeddings and load them into a dictionary.\n",
    "# Example: {\"word\": embedding_vector}\n",
    "\n",
    "# Class signature example:\n",
    "# class WordContextDataset(Dataset):\n",
    "#     def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
    "#         # Your code here\n",
    "#         pass\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         # Your code here\n",
    "#         pass\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         # Your code here\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa6d07d9-4853-4e9d-b8c9-5391e97ad255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 500)\n",
      "X_val shape: (5000, 500)\n",
      "X_test shape: (25000, 500)\n",
      "Testing parameters: {'batch_size': 32, 'epochs': 5, 'filters': 64, 'kernel_initializer': 'glorot_uniform', 'kernel_size': 3, 'optimizer': 'adam', 'rate': 0.25, 'units': 128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Sequential model 'sequential' has no defined outputs yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m earlystopper \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     90\u001b[0m     X_train, y_train,\n\u001b[1;32m     91\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_val, y_val),\n\u001b[1;32m     92\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[earlystopper],\n\u001b[1;32m     93\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     94\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Store the validation accuracy\u001b[39;00m\n\u001b[1;32m     98\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scikeras/wrappers.py:1501\u001b[0m, in \u001b[0;36mKerasClassifier.fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m sample_weight\n\u001b[1;32m   1500\u001b[0m     sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_weight, y\u001b[38;5;241m=\u001b[39my)\n\u001b[0;32m-> 1501\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scikeras/wrappers.py:770\u001b[0m, in \u001b[0;36mBaseWrapper.fit\u001b[0;34m(self, X, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    765\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit__epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs)\n\u001b[1;32m    767\u001b[0m )\n\u001b[1;32m    768\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 770\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    771\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m    772\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[1;32m    773\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[1;32m    774\u001b[0m     warm_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start,\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    776\u001b[0m )\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scikeras/wrappers.py:936\u001b[0m, in \u001b[0;36mBaseWrapper._fit\u001b[0;34m(self, X, y, sample_weight, warm_start, epochs, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    933\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_encoder_\u001b[38;5;241m.\u001b[39mtransform(y)\n\u001b[1;32m    934\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_encoder_\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m--> 936\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_model_compatibility(y)\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_keras_model(\n\u001b[1;32m    939\u001b[0m     X,\n\u001b[1;32m    940\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    946\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/scikeras/wrappers.py:559\u001b[0m, in \u001b[0;36mBaseWrapper._check_model_compatibility\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# check if this is a multi-output model\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_outputs_expected_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;66;03m# n_outputs_expected_ is generated by data transformers\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;66;03m# we recognize the attribute but do not force it to be\u001b[39;00m\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;66;03m# generated\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_expected_ \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39moutputs):\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    561\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected a Keras model input of size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    562\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_expected_\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_\u001b[38;5;241m.\u001b[39moutputs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# check that if the user gave us a loss function it ended up in\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# the actual model\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/keras/src/models/sequential.py:292\u001b[0m, in \u001b[0;36mSequential.outputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functional:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functional\u001b[38;5;241m.\u001b[39moutputs\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequential model \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no defined outputs yet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Sequential model 'sequential' has no defined outputs yet."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "vocab_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Split training and validation sets\n",
    "X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# Pad sequences\n",
    "maximum_sequence_length = 500\n",
    "X_train = pad_sequences(X_train, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "X_val = pad_sequences(X_val, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "X_test = pad_sequences(X_test, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "# Define the model creation function\n",
    "def create_model(filters=64, kernel_size=3, strides=1, units=256, \n",
    "                 optimizer='adam', rate=0.25, kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=16, input_length=maximum_sequence_length))\n",
    "\n",
    "    # Convolutional Layer(s)\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                     padding='same', activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # Dense layer(s)\n",
    "    model.add(Dense(units=units, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(rate))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Single unit for binary classification\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Example: pass the parameters directly during initialization\n",
    "model = KerasClassifier(model=create_model,\n",
    "                        filters=128,\n",
    "                        kernel_size=5,\n",
    "                        optimizer='adam',\n",
    "                        units=128,\n",
    "                        rate=0.25,\n",
    "                        kernel_initializer='TruncatedNormal',\n",
    "                        verbose=1)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\n",
    "    'filters': [64, 128],\n",
    "    'kernel_size': [3, 5],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'units': [128, 256],\n",
    "    'rate': [0.25, 0.5],\n",
    "    'kernel_initializer': ['glorot_uniform', 'he_normal'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Iterate over grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "param_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Initialize the model with the parameters from the grid\n",
    "    model = KerasClassifier(model=create_model, **params, verbose=1)\n",
    "    \n",
    "    # Early stopping\n",
    "    earlystopper = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1)\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopper],\n",
    "        epochs=params['epochs'],\n",
    "        batch_size=params['batch_size']\n",
    "    )\n",
    "    \n",
    "    # Store the validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    param_scores.append(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "# Select the best parameters\n",
    "best_index = np.argmax(param_scores)\n",
    "best_params = list(grid)[best_index]\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {param_scores[best_index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf77713-74e7-40c7-b50d-5f42dd28466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Define and train the model\n",
    "# Define a neural network for sentiment classification using PyTorch.\n",
    "# The network should take an input vector of concatenated word and context embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d625da-c09a-443b-8c2c-795e01d750d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example:\n",
    "# class SentimentClassifier(nn.Module):\n",
    "#     def __init__(self, input_dim):\n",
    "#         super(SentimentClassifier, self).__init__()\n",
    "#         # Your code here\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # Your code here\n",
    "#         pass\n",
    "\n",
    "# Implement a training loop to train the model on the dataset created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beed1f0-9b85-4121-801f-b8217996f87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Evaluate the model\n",
    "# Evaluate the trained model on a validation set.\n",
    "# Use metrics such as precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df935c85-b28c-4b88-9d14-9104095272d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code to evaluate the model:\n",
    "# with torch.no_grad():\n",
    "#     # Predict on validation data and calculate metrics\n",
    "#     pass\n",
    "\n",
    "# Optional: Experiment with hyperparameters or model architecture to improve performance.\n",
    "# Examples: Try different window sizes, embedding dimensions, or additional layers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e68e80-d3ef-48b1-8d41-efc3be9c4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dropout, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "vocab_size = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "# Split training and validation sets\n",
    "X_train, X_val = X_train[:-5000], X_train[-5000:]\n",
    "y_train, y_val = y_train[:-5000], y_train[-5000:]\n",
    "\n",
    "# Pad sequences\n",
    "maximum_sequence_length = 500\n",
    "X_train = pad_sequences(X_train, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "X_val = pad_sequences(X_val, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "X_test = pad_sequences(X_test, value=0, padding='post', maxlen=maximum_sequence_length)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c123b26b-31c2-409e-b978-ce8c0fa2f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model creation function\n",
    "def create_model(filters=64, kernel_size=3, strides=1, units=256, \n",
    "                 optimizer='adam', rate=0.25, kernel_initializer='glorot_uniform'):\n",
    "    model = Sequential()\n",
    "    # Embedding layer\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=16, input_length=maximum_sequence_length))\n",
    "\n",
    "    # Convolutional Layer(s)\n",
    "    model.add(Dropout(rate))\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, \n",
    "                     padding='same', activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # Dense layer(s)\n",
    "    model.add(Dense(units=units, activation='relu', kernel_initializer=kernel_initializer))\n",
    "    model.add(Dropout(rate))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Single unit for binary classification\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model using KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Example: pass the parameters directly during initialization\n",
    "model = KerasClassifier(model=create_model,\n",
    "                        filters=128,\n",
    "                        kernel_size=5,\n",
    "                        optimizer='adam',\n",
    "                        units=128,\n",
    "                        rate=0.25,\n",
    "                        kernel_initializer='TruncatedNormal',\n",
    "                        verbose=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4684f7-5aff-4dd9-9821-14a12b1316ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {\n",
    "    'filters': [64, 128],\n",
    "    'kernel_size': [3, 5],\n",
    "    'optimizer': ['adam', 'rmsprop'],\n",
    "    'units': [128, 256],\n",
    "    'rate': [0.25, 0.5],\n",
    "    'kernel_initializer': ['glorot_uniform', 'he_normal'],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [5, 10]\n",
    "}\n",
    "\n",
    "# Iterate over grid\n",
    "grid = ParameterGrid(param_grid)\n",
    "param_scores = []\n",
    "\n",
    "for params in grid:\n",
    "    print(f\"Testing parameters: {params}\")\n",
    "    \n",
    "    # Initialize the model with the parameters from the grid\n",
    "    model = KerasClassifier(model=create_model, **params, verbose=1)\n",
    "    \n",
    "    # Early stopping\n",
    "    earlystopper = EarlyStopping(monitor='val_accuracy', patience=2, verbose=1)\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[earlystopper]\n",
    "    )\n",
    "    \n",
    "    # Store the validation accuracy\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    param_scores.append(val_accuracy)\n",
    "    print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "    print('-' * 80)\n",
    "\n",
    "# Select the best parameters\n",
    "best_index = np.argmax(param_scores)\n",
    "best_params = list(grid)[best_index]\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Validation Accuracy: {param_scores[best_index]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2817e1-2228-448c-8011-15eda1ce8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(**best_params)\n",
    "model.fit(np.vstack((X_train, X_val)), np.hstack((y_train, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb30b6e-b02b-44fe-a8e6-1ee9b2450b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
