{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual Word Sentiment Classification\n",
    "#### By Wael Ben Slima and Marko Babic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd    # to load dataset\n",
    "import numpy as np     # for mathematic equation\n",
    "from nltk.corpus import stopwords   # to get collection of stopwords\n",
    "from sklearn.model_selection import train_test_split       # for splitting dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # to encode text to int\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences   # to do padding or truncating\n",
    "from tensorflow.keras.models import Sequential     # the model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense # layers of the architecture\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint   # save model\n",
    "from tensorflow.keras.models import load_model   # load saved model\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Stop Word</b> is a commonly used words in a sentence, usually a search engine is programmed to ignore this words (i.e. \"the\", \"a\", \"an\", \"of\", etc.)\n",
    "\n",
    "<i>Declaring the english stop words</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### TASK 1Load and Clean Dataset\n",
    "\n",
    "In the original dataset, the reviews are still dirty. There are still html tags, numbers, uppercase, and punctuations. This will not be good for training, so in <b>load_dataset()</b> function, beside loading the dataset using <b>pandas</b>, I also pre-process the reviews by removing html tags, non alphabet (punctuations and numbers), stop words, and lower case all of the reviews.\n",
    "\n",
    "### Encode Sentiments\n",
    "In the same function, I also encode the sentiments into integers (0 and 1). Where 0 is for negative sentiments and 1 is for positive sentiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews\n",
      "0        [one, reviewers, mentioned, watching, oz, epis...\n",
      "1        [a, wonderful, little, production, the, filmin...\n",
      "2        [i, thought, wonderful, way, spend, time, hot,...\n",
      "3        [basically, family, little, boy, jake, thinks,...\n",
      "4        [petter, mattei, love, time, money, visually, ...\n",
      "                               ...                        \n",
      "49995    [i, thought, movie, right, good, job, it, crea...\n",
      "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
      "49997    [i, catholic, taught, parochial, elementary, s...\n",
      "49998    [i, going, disagree, previous, comment, side, ...\n",
      "49999    [no, one, expects, star, trek, movies, high, a...\n",
      "Name: review, Length: 50000, dtype: object \n",
      "\n",
      "Sentiment\n",
      "0        1\n",
      "1        1\n",
      "2        1\n",
      "3        0\n",
      "4        1\n",
      "        ..\n",
      "49995    1\n",
      "49996    0\n",
      "49997    0\n",
      "49998    0\n",
      "49999    0\n",
      "Name: sentiment, Length: 50000, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB\\AppData\\Local\\Temp\\ipykernel_14372\\1137108278.py:14: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  y_data = y_data.replace('negative', 0)\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    df = pd.read_csv('IMDB Dataset.csv')\n",
    "    x_data = df['review']       # Reviews/Input\n",
    "    y_data = df['sentiment']    # Sentiment/Output\n",
    "\n",
    "    # PRE-PROCESS REVIEW\n",
    "    x_data = x_data.replace({'<.*?>': ''}, regex = True)          # remove html tag\n",
    "    x_data = x_data.replace({'[^A-Za-z]': ' '}, regex = True)     # remove non alphabet\n",
    "    x_data = x_data.apply(lambda review: [w for w in review.split() if w not in english_stops])  # remove stop words\n",
    "    x_data = x_data.apply(lambda review: [w.lower() for w in review])   # lower case\n",
    "    \n",
    "    # ENCODE SENTIMENT -> 0 & 1\n",
    "    y_data = y_data.replace('positive', 1)\n",
    "    y_data = y_data.replace('negative', 0)\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "x_data, y_data = load_dataset()\n",
    "\n",
    "print('Reviews')\n",
    "print(x_data, '\\n')\n",
    "print('Sentiment')\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Dataset\n",
    "In this work, I decided to split the data into 80% of Training and 20% of Testing set using <b>train_test_split</b> method from Scikit-Learn. By using this method, it automatically shuffles the dataset. We need to shuffle the data because in the original dataset, the reviews and sentiments are in order, where they list positive reviews first and then negative reviews. By shuffling the data, it will be distributed equally in the model, so it will be more accurate for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set\n",
      "22842    [ty, cobb, far, interesting, belligerently, in...\n",
      "25265    [i, originally, gave, episode, rating, two, i,...\n",
      "15386    [there, many, different, versions, one, floati...\n",
      "6706     [there, really, one, thing, say, sorry, movie,...\n",
      "15551    [this, another, one, movies, could, great, the...\n",
      "                               ...                        \n",
      "22546    [my, wife, i, rented, movie, people, drawn, pa...\n",
      "19941    [this, movie, amazing, i, tears, end, movie, o...\n",
      "1051     [i, rented, movie, friend, good, laugh, we, ac...\n",
      "2156     [a, humorous, voyage, normally, somber, funera...\n",
      "28248    [it, really, cheesy, parody, tomb, raider, ind...\n",
      "Name: review, Length: 40000, dtype: object \n",
      "\n",
      "5944     [excellent, film, suzy, kendall, hold, interes...\n",
      "35724    [i, agree, jerry, it, underrated, space, movie...\n",
      "11089    [i, found, film, one, great, heart, warming, g...\n",
      "9832     [i, rented, film, yesterday, mostly, due, good...\n",
      "44598    [not, one, films, one, biggest, pieces, tripe,...\n",
      "                               ...                        \n",
      "46615    [i, smiled, whole, film, the, music, great, th...\n",
      "41083    [it, one, best, movies, i, seen, year, i, agre...\n",
      "11575    [quote, nicolas, martin, nicmart, houston, tx,...\n",
      "16925    [great, music, people, pathetic, a, true, peri...\n",
      "40617    [this, movie, honestly, one, greatest, movies,...\n",
      "Name: review, Length: 10000, dtype: object \n",
      "\n",
      "Test Set\n",
      "22842    0\n",
      "25265    0\n",
      "15386    0\n",
      "6706     0\n",
      "15551    0\n",
      "        ..\n",
      "22546    0\n",
      "19941    1\n",
      "1051     0\n",
      "2156     1\n",
      "28248    0\n",
      "Name: sentiment, Length: 40000, dtype: int64 \n",
      "\n",
      "5944     1\n",
      "35724    1\n",
      "11089    1\n",
      "9832     0\n",
      "44598    0\n",
      "        ..\n",
      "46615    1\n",
      "41083    1\n",
      "11575    1\n",
      "16925    0\n",
      "40617    0\n",
      "Name: sentiment, Length: 10000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size = 0.2)\n",
    "\n",
    "print('Train Set')\n",
    "print(x_train, '\\n')\n",
    "print(x_test, '\\n')\n",
    "print('Test Set')\n",
    "print(y_train, '\\n')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<i>Function for getting the maximum review length, by calculating the mean of all the reviews length (using <b>numpy.mean</b>)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length():\n",
    "    review_length = []\n",
    "    for review in x_train:\n",
    "        review_length.append(len(review))\n",
    "\n",
    "    return int(np.ceil(np.mean(review_length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### TASK 2 : Tokenize and Pad/Truncate Reviews\n",
    "A Neural Network only accepts numeric data, so we need to encode the reviews. I use <b>tensorflow.keras.preprocessing.text.Tokenizer</b> to encode the reviews into integers, where each unique word is automatically indexed (using <b>fit_on_texts</b> method) based on <b>x_train</b>. <br>\n",
    "<b>x_train</b> and <b>x_test</b> is converted into integers using <b>texts_to_sequences</b> method.\n",
    "\n",
    "Each reviews has a different length, so we need to add padding (by adding 0) or truncating the words to the same length (in this case, it is the mean of all reviews length) using <b>tensorflow.keras.preprocessing.sequence.pad_sequences</b>.\n",
    "\n",
    "\n",
    "<b>post</b>, pad or truncate the words in the back of a sentence<br>\n",
    "<b>pre</b>, pad or truncate the words in front of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded X Train\n",
      " [[12613  6826   132 ...    42    14   893]\n",
      " [    1  1763   407 ...    76     6  1116]\n",
      " [   49    38   182 ...   813  4779   894]\n",
      " ...\n",
      " [    1  1514     3 ...     0     0     0]\n",
      " [   39  1902  5151 ...     0     0     0]\n",
      " [    7    13   854 ...     0     0     0]] \n",
      "\n",
      "Encoded X Test\n",
      " [[  224     4 10946 ...     0     0     0]\n",
      " [    1   926  1555 ...     0     0     0]\n",
      " [    1   161     4 ...     0     0     0]\n",
      " ...\n",
      " [ 3033  5999  1326 ...     0     0     0]\n",
      " [   20   115    21 ...     0     0     0]\n",
      " [    8     3  1147 ... 18922    10   280]] \n",
      "\n",
      "Maximum review length:  130\n"
     ]
    }
   ],
   "source": [
    "# ENCODE REVIEW\n",
    "token = Tokenizer(lower=False)    # no need lower, because already lowered the data in load_data()\n",
    "token.fit_on_texts(x_train)\n",
    "x_train = token.texts_to_sequences(x_train)\n",
    "x_test = token.texts_to_sequences(x_test)\n",
    "\n",
    "max_length = get_max_length()\n",
    "\n",
    "x_train = pad_sequences(x_train, maxlen=max_length, padding='post', truncating='post')\n",
    "x_test = pad_sequences(x_test, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "total_words = len(token.word_index) + 1   # add 1 because of 0 padding\n",
    "\n",
    "print('Encoded X Train\\n', x_train, '\\n')\n",
    "print('Encoded X Test\\n', x_test, '\\n')\n",
    "print('Maximum review length: ', max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3 Build Architecture/Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe embeddings loaded successfully!\n",
      "Dataset size: 13\n",
      "Target word: the\n",
      "Target embedding: [ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n",
      "Context embedding: [ 0.08685999  0.25556     0.09003502  0.38064998  0.61828595  0.252505\n",
      " -0.44690502  0.1213905  -0.6212535  -0.2446      0.12698    -0.678855\n",
      " -0.856985   -0.07613    -0.01766001 -0.090982    0.01950999 -0.06645\n",
      " -0.41999498  0.38361    -0.23531502  0.21082501 -0.075105   -0.056531\n",
      " -0.258795   -2.0649     -0.07715999 -0.41225502 -0.57509506  0.691222\n",
      "  3.20775    -0.173415   -0.905717   -0.15075    -0.022085   -0.3438775\n",
      " -0.1478155   0.15257001 -0.06367999  0.09089001 -0.2884875   0.3711264\n",
      " -0.642058   -0.559675    0.20099999  0.2365185  -0.79858    -0.309185\n",
      "  0.12181351 -0.50999   ]\n",
      "Score: 0\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load GloVe embeddings into a dictionary\n",
    "def load_glove_embeddings(url, embedding_dim=50):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "            with zip_ref.open(f'glove.6B.{embedding_dim}d.txt') as f:\n",
    "                embedding_model = {}\n",
    "                for line in f:\n",
    "                    values = line.decode('utf-8').split()\n",
    "                    word = values[0]\n",
    "                    embedding_vector = np.array(values[1:], dtype='float32')\n",
    "                    embedding_model[word] = embedding_vector\n",
    "                print(\"GloVe embeddings loaded successfully!\")\n",
    "                return embedding_model\n",
    "    else:\n",
    "        print(f\"Failed to download the GloVe file. Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Define the dataset class\n",
    "class WordContextDataset(Dataset):\n",
    "    def __init__(self, df, word_scores, embedding_model, window_size=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - df: A pandas DataFrame containing the text data.\n",
    "        - word_scores: A dictionary containing words and their corresponding scores.\n",
    "        - embedding_model: A dictionary containing word embeddings (e.g., GloVe).\n",
    "        - window_size: Size of the context window around the target word.\n",
    "        \"\"\"\n",
    "        if embedding_model is None:\n",
    "            raise ValueError(\"Embedding model is None. Please check if the GloVe embeddings were loaded correctly.\")\n",
    "        \n",
    "        self.texts = df['text'].tolist()\n",
    "        self.word_scores = word_scores\n",
    "        self.embedding_model = embedding_model\n",
    "        self.window_size = window_size\n",
    "        self.embedding_dim = next(iter(self.embedding_model.values())).shape[0]\n",
    "        self.data = self._prepare_dataset()\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        data = []\n",
    "        for text in self.texts:\n",
    "            tokens = text.split()\n",
    "            for i, target_word in enumerate(tokens):\n",
    "                if target_word not in self.embedding_model:\n",
    "                    continue\n",
    "                \n",
    "                # Get context window\n",
    "                start = max(i - self.window_size, 0)\n",
    "                end = min(i + self.window_size + 1, len(tokens))\n",
    "                context_words = tokens[start:i] + tokens[i+1:end]\n",
    "\n",
    "                # Compute embeddings\n",
    "                target_embedding = self.embedding_model.get(target_word, np.zeros(self.embedding_dim))\n",
    "                context_embeddings = [\n",
    "                    self.embedding_model.get(word, np.zeros(self.embedding_dim)) \n",
    "                    for word in context_words\n",
    "                ]\n",
    "                if context_embeddings:\n",
    "                    averaged_context_embedding = np.mean(context_embeddings, axis=0)\n",
    "                else:\n",
    "                    averaged_context_embedding = np.zeros(self.embedding_dim)\n",
    "                \n",
    "                # Add to dataset\n",
    "                score = self.word_scores.get(target_word, 0)\n",
    "                data.append({\n",
    "                    'target_word': target_word,\n",
    "                    'target_embedding': target_embedding,\n",
    "                    'context_embedding': averaged_context_embedding,\n",
    "                    'score': score\n",
    "                })\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Corrected URL\n",
    "    url = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "\n",
    "    # Load GloVe embeddings\n",
    "    embedding_model = load_glove_embeddings(url, 50)\n",
    "    \n",
    "    # Check if embeddings were loaded\n",
    "    if embedding_model is not None:\n",
    "        # Example DataFrame\n",
    "        data = {\n",
    "            \"text\": [\n",
    "                \"the leader in natural language processing\",\n",
    "                \"deep learning is a prime example of AI\"\n",
    "            ]\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Example word scores\n",
    "        word_scores = {\n",
    "            \"leader\": 1.0,\n",
    "            \"natural\": 0.9,\n",
    "            \"language\": 0.8,\n",
    "            \"deep\": 0.7,\n",
    "            \"learning\": 0.6\n",
    "        }\n",
    "\n",
    "        # Create the dataset\n",
    "        dataset = WordContextDataset(df, word_scores, embedding_model, window_size=2)\n",
    "\n",
    "        # Example access\n",
    "        print(f\"Dataset size: {len(dataset)}\")\n",
    "        sample = dataset[0]\n",
    "        print(f\"Target word: {sample['target_word']}\")\n",
    "        print(f\"Target embedding: {sample['target_embedding']}\")\n",
    "        print(f\"Context embedding: {sample['context_embedding']}\")\n",
    "        print(f\"Score: {sample['score']}\")\n",
    "    else:\n",
    "        print(\"Failed to create dataset due to missing embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### TASK 4 Build Architecture/Model\n",
    "<b>Embedding Layer</b>: in simple terms, it creates word vectors of each word in the <i>word_index</i> and group words that are related or have similar meaning by analyzing other words around them.\n",
    "\n",
    "<b>LSTM Layer</b>: to make a decision to keep or throw away data by considering the current input, previous output, and previous memory. There are some important components in LSTM.\n",
    "<ul>\n",
    "    <li><b>Forget Gate</b>, decides information is to be kept or thrown away</li>\n",
    "    <li><b>Input Gate</b>, updates cell state by passing previous output and current input into sigmoid activation function</li>\n",
    "    <li><b>Cell State</b>, calculate new cell state, it is multiplied by forget vector (drop value if multiplied by a near 0), add it with the output from input gate to update the cell state value.</li>\n",
    "    <li><b>Ouput Gate</b>, decides the next hidden state and used for predictions</li>\n",
    "</ul>\n",
    "\n",
    "<b>Dense Layer</b>: compute the input with the weight matrix and bias (optional), and using an activation function. I use <b>Sigmoid</b> activation function for this work because the output is only 0 or 1.\n",
    "\n",
    "The optimizer is <b>Adam</b> and the loss function is <b>Binary Crossentropy</b> because again the output is only 0 and 1, which is a binary number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# ARCHITECTURE\n",
    "EMBED_DIM = 32\n",
    "LSTM_OUT = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, EMBED_DIM, input_length = max_length))\n",
    "model.add(LSTM(LSTM_OUT))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Training\n",
    "For training, it is simple. We only need to fit our <b>x_train</b> (input) and <b>y_train</b> (output/label) data. For this training, I use a mini-batch learning method with a <b>batch_size</b> of <i>128</i> and <i>5</i> <b>epochs</b>.\n",
    "\n",
    "Also, I added a callback called **checkpoint** to save the model locally for every epoch if its accuracy improved from the previous epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    'models/LSTM.keras',\n",
    "    monitor='accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5284 - loss: 0.6845\n",
      "Epoch 1: accuracy improved from -inf to 0.58938, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.5286 - loss: 0.6844\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7568 - loss: 0.5470\n",
      "Epoch 2: accuracy improved from 0.58938 to 0.76050, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 75ms/step - accuracy: 0.7568 - loss: 0.5470\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.6618 - loss: 0.6286\n",
      "Epoch 3: accuracy did not improve from 0.76050\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 73ms/step - accuracy: 0.6616 - loss: 0.6287\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6952 - loss: 0.5980\n",
      "Epoch 4: accuracy did not improve from 0.76050\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 77ms/step - accuracy: 0.6953 - loss: 0.5980\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7758 - loss: 0.4802\n",
      "Epoch 5: accuracy improved from 0.76050 to 0.82832, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 84ms/step - accuracy: 0.7759 - loss: 0.4800\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9154 - loss: 0.2348\n",
      "Epoch 6: accuracy improved from 0.82832 to 0.91658, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 84ms/step - accuracy: 0.9154 - loss: 0.2348\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.9546 - loss: 0.1419\n",
      "Epoch 7: accuracy improved from 0.91658 to 0.95130, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 95ms/step - accuracy: 0.9546 - loss: 0.1419\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9716 - loss: 0.1011\n",
      "Epoch 8: accuracy improved from 0.95130 to 0.96880, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 85ms/step - accuracy: 0.9716 - loss: 0.1011\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9805 - loss: 0.0726\n",
      "Epoch 9: accuracy improved from 0.96880 to 0.97932, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 79ms/step - accuracy: 0.9805 - loss: 0.0726\n",
      "Epoch 10/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9881 - loss: 0.0493\n",
      "Epoch 10: accuracy improved from 0.97932 to 0.98615, saving model to models/LSTM.keras\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 85ms/step - accuracy: 0.9881 - loss: 0.0493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c3dc713a40>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 128, epochs = 10, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Testing\n",
    "To evaluate the model, we need to predict the sentiment using our <b>x_test</b> data and comparing the predictions with <b>y_test</b> (expected output) data. Then, we calculate the accuracy of the model by dividing numbers of correct prediction with the total data. Resulted an accuracy of <b>86.63%</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 37ms/step\n",
      "Correct Prediction: 4994\n",
      "Wrong Prediction: 5006\n",
      "Accuracy: 49.94\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 使用 model.predict() 方法进行预测\n",
    "y_pred = model.predict(x_test, batch_size=128)\n",
    "\n",
    "# 使用 np.argmax() 函数获取预测结果的索引值\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "true = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y == y_pred_classes[i]:\n",
    "        true += 1\n",
    "\n",
    "print('Correct Prediction: {}'.format(true))\n",
    "print('Wrong Prediction: {}'.format(len(y_pred_classes) - true))\n",
    "print('Accuracy: {}'.format(true/len(y_pred_classes)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load Saved Model\n",
    "\n",
    "Load saved model and use it to predict a movie review statement's sentiment (positive or negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model('models/LSTM.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Receives a review as an input to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Movie Review:  I like cookies\n"
     ]
    }
   ],
   "source": [
    "review = str(input('Movie Review: '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input must be pre processed before it is passed to the model to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned:  I like cookies\n",
      "Filtered:  ['i like cookies']\n"
     ]
    }
   ],
   "source": [
    "# Pre-process input\n",
    "regex = re.compile(r'[^a-zA-Z\\s]')\n",
    "review = regex.sub('', review)\n",
    "print('Cleaned: ', review)\n",
    "\n",
    "words = review.split(' ')\n",
    "filtered = [w for w in words if w not in english_stops]\n",
    "filtered = ' '.join(filtered)\n",
    "filtered = [filtered.lower()]\n",
    "\n",
    "print('Filtered: ', filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we need to tokenize and encode the words. I use the tokenizer which was previously declared because we want to encode the words based on words that are known by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    1     6 16274     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "tokenize_words = token.texts_to_sequences(filtered)\n",
    "tokenize_words = pad_sequences(tokenize_words, maxlen=max_length, padding='post', truncating='post')\n",
    "print(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the prediction which shows the **confidence score** of the review statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 204ms/step\n",
      "[[0.9371112]]\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(tokenize_words)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the confidence score is close to 0, then the statement is **negative**. On the other hand, if the confidence score is close to 1, then the statement is **positive**. I use a threshold of **0.7** to determine which confidence score is positive and negative, so if it is equal or greater than 0.7, it is **positive** and if it is less than 0.7, it is **negative**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "if result >= 0.7:\n",
    "    print('positive')\n",
    "else:\n",
    "    print('negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difficulties Encountered\n",
    "- One of the main difficulties was the technical issue with TorchText and Python version compatibility. This required a shift in strategy to use Kaggle and Pandas for data loading, which was not the initial plan.\n",
    "- Problem of datas that had different shape\n",
    "- Problem related to keras : in the function to fit the model, we are using sequential model and it says sequential has no outputs yet. Therefore, we tried to look for the issue on stackoverflow and github but we did not get response so we continued to debug it.\n",
    "\n",
    "#### Steps taken to alleviate difficulties\n",
    "To address the TorchText and Python version compatibility issue, we opted to use Kaggle to directly download the dataset and then import it into Pandas for preprocessing. This change in strategy allowed us to bypass the initial dependency from TorchText. For the challenge of data with varying shapes, I implemented data preprocessing techniques such as padding and truncation to normalize the input data, ensuring that all sequences were of equal length and suitable for model training. The website stack helped also while we were facing errors.\n",
    "\n",
    "#### General description of what you did, explain how you understood the task and what you did to solve it in general language, no code.\n",
    "For this task, we built a model that could classify the sentiment of individual words based on the sentiment of the sentences they appear in. To do so, we first prepared the data by tokenizing the text into words and sentences, and then padded the sentences to a uniform length to standardize the input for the model. Then, we developed a neural network model designed to process sequences of words and learn the contextual relationships that influence word sentiment. The model was trained on the IMDb dataset, using the sentence-level sentiment labels to teach it to classify words as positive, negative, or neutral. After training, the model was evaluated to assess its ability to accurately classify word sentiments in new sentences.\n",
    "\n",
    "#### Potential limitations of our approach, what could be issues, how could this be hard on different data or with slightly different conditions\n",
    "- **Data Bias**: The IMDb dataset may contain biases that could affect the model's performance on different types of data or under slightly different conditions.\n",
    "- **Context Length**: The model might struggle with longer contexts or sentences where the relationship between words and sentiment is more complex.\n",
    "- **Vocabulary Size**: Limitations in the model's vocabulary size could lead to misclassifications of words that are not well-represented in the training data.\n",
    "\n",
    "#### Extensions and Future Work\n",
    "An interesting extension of this work could involve exploring different neural network architectures, such as transformers, which are known for their ability to handle long-range dependencies in text. Additionally, incorporating multimodal data, such as movie scenes or audio, could provide richer context for sentiment analysis. \n",
    "- This approach could also be extended to detect hate speech or discriminatory comments on social media platforms more effectively.\n",
    "- For hotel, restaurant and product that base their grade(with stars usuall) on the positive review.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
